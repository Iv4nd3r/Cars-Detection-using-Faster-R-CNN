{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"TPU","colab":{"collapsed_sections":["6nhVWEFQdF8L","hOLAkDbSf9gE","VorHfLMORKin"],"gpuType":"V28","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2903151,"sourceType":"datasetVersion","datasetId":786319},{"sourceId":12030982,"sourceType":"datasetVersion","datasetId":7569722}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ivanderwijaya/cars-detection-using-faster-r-cnn?scriptVersionId=246459958\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"#Cars Detection using Faster R-CNN\n\nIn this notebook, we will doing object detection on [Self Driving Cars Dataset](https://www.kaggle.com/datasets/alincijov/self-driving-cars/data) to detect cars in the images using Faster R-CNN model.\n\nIn this notebook, we will use Faster R-CNN model with this configuration:\n","metadata":{"id":"ScgLA6O5ac9T"}},{"cell_type":"markdown","source":"## 1. Import Self Driving Cars Dataset from Kaggle","metadata":{"id":"6nhVWEFQdF8L"}},{"cell_type":"code","source":"# Install required dependencies\n\n!pip install opendatasets\n!pip install pandas","metadata":{"id":"Gl0iQVxXaa7x","outputId":"13e975e8-a0dc-4f81-da62-12f64766bcc1","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:46:57.008763Z","iopub.execute_input":"2025-06-02T02:46:57.008995Z","iopub.status.idle":"2025-06-02T02:47:06.186718Z","shell.execute_reply.started":"2025-06-02T02:46:57.008976Z","shell.execute_reply":"2025-06-02T02:47:06.185584Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import opendatasets as od\nimport pandas as pd\n\n# od.download(\"https://www.kaggle.com/datasets/alincijov/self-driving-cars/data\") #Uncomment this if you are running this besides on Kaggle Notebook\n\nimport kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"alincijov/self-driving-cars\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"id":"a-0Kzb_IcrHe","outputId":"e9fe36d6-d662-42a6-d317-e39c9e982ed2","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:06.188164Z","iopub.execute_input":"2025-06-02T02:47:06.188888Z","iopub.status.idle":"2025-06-02T02:47:06.758528Z","shell.execute_reply.started":"2025-06-02T02:47:06.188846Z","shell.execute_reply":"2025-06-02T02:47:06.757649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file =('/kaggle/input/self-driving-cars/labels_train.csv') #Adjust the file path if you are running this besides on kaggle\nnewData = pd.read_csv(file)\nnewData.head()","metadata":{"id":"22VfuSLWc2Bx","outputId":"0e46f430-fd7c-46be-84b5-5235945826ff","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:06.76069Z","iopub.execute_input":"2025-06-02T02:47:06.760946Z","iopub.status.idle":"2025-06-02T02:47:07.014952Z","shell.execute_reply.started":"2025-06-02T02:47:06.760926Z","shell.execute_reply":"2025-06-02T02:47:07.014045Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Data Processing","metadata":{"id":"hOLAkDbSf9gE"}},{"cell_type":"code","source":"# Check for empty dataset\nif newData.empty:\n  print(\"Dataset is empty. No data to process or train.\")\nelse:\n  print(\"Dataset loaded successfully with\", len(newData), \"rows.\")","metadata":{"id":"mss9AdWvf9KJ","outputId":"b708beee-36d8-48c8-b84e-4a2cc4485d5d","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:07.016414Z","iopub.execute_input":"2025-06-02T02:47:07.017381Z","iopub.status.idle":"2025-06-02T02:47:07.022039Z","shell.execute_reply.started":"2025-06-02T02:47:07.017349Z","shell.execute_reply":"2025-06-02T02:47:07.021276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for redundancy (duplicate rows)\n# Use keep=False to mark all duplicate occurrences as True\nduplicate_mask = newData.duplicated(keep=False)\nduplicate_count = duplicate_mask.sum()\n\nif duplicate_count > 0:\n  print(\"Found\", duplicate_count, \"duplicate rows. Removing duplicates.\")\n  print(\"Showing duplicate rows:\")\n  # Show data duplicates\n  display(newData[duplicate_mask]) # Use display for better formatting in Colab\n  # Remove duplicates\n  newData.drop_duplicates(inplace=True)\n  print(\"Dataset now has\", len(newData), \"rows after removing duplicates.\")\nelse:\n  print(\"No duplicate rows found.\")","metadata":{"id":"FZKnTo2gh7pH","outputId":"9ccbeb5f-16d6-4ad4-d615-47ffe8784427","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:07.022945Z","iopub.execute_input":"2025-06-02T02:47:07.023268Z","iopub.status.idle":"2025-06-02T02:47:07.123597Z","shell.execute_reply.started":"2025-06-02T02:47:07.023238Z","shell.execute_reply":"2025-06-02T02:47:07.122486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values\nmissing_values = newData.isnull().sum()\nif missing_values.sum() > 0:\n  print(\"\\nMissing values per column:\")\n  print(missing_values[missing_values > 0])\nelse:\n  print(\"\\nNo missing values found.\")","metadata":{"id":"_XjWMxcbh8bk","outputId":"b4142453-f45d-42d7-9a7a-fd31e5e57da1","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:07.124425Z","iopub.execute_input":"2025-06-02T02:47:07.124673Z","iopub.status.idle":"2025-06-02T02:47:07.14441Z","shell.execute_reply.started":"2025-06-02T02:47:07.124652Z","shell.execute_reply":"2025-06-02T02:47:07.143373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for invalid bounding box dimensions (width or height <= 0)\ninvalid_bbox_mask = (newData['xmax'] <= newData['xmin']) | (newData['ymax'] <= newData['ymin'])\ninvalid_bbox_count = invalid_bbox_mask.sum()\n\nif invalid_bbox_count > 0:\n    print(\"\\nFound\", invalid_bbox_count, \"rows with invalid bounding box dimensions.\")\n    print(\"Showing invalid rows:\")\n    display(newData[invalid_bbox_mask])\n    print(\"Removing rows with invalid bounding box dimensions.\")\n    newData = newData[~invalid_bbox_mask].copy() # Use .copy() to avoid SettingWithCopyWarning\n    print(\"Dataset now has\", len(newData), \"rows after removing invalid bounding boxes.\")\nelse:\n    print(\"\\nNo rows with invalid bounding box dimensions found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:07.145587Z","iopub.execute_input":"2025-06-02T02:47:07.146124Z","iopub.status.idle":"2025-06-02T02:47:07.181094Z","shell.execute_reply.started":"2025-06-02T02:47:07.146076Z","shell.execute_reply":"2025-06-02T02:47:07.180086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nAnalyzing class distribution...\")\n\nclass_counts = newData['class_id'].value_counts()\n\nprint(\"Class distribution:\")\nprint(class_counts)\n\n# You can visualize this distribution with a bar plot\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nclass_counts.plot(kind='bar')\nplt.title('Distribution of Classes')\nplt.xlabel('Class')\nplt.ylabel('Count')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n# Analyze for imbalance\nif class_counts.min() / class_counts.max() < 0.1: # Example threshold for imbalance\n    print(\"\\nPotential class imbalance detected. Some classes have significantly fewer examples.\")\n    print(\"Consider data augmentation or re-sampling techniques during training.\")","metadata":{"id":"lPghffnnP6cJ","outputId":"af42c27f-ffbd-4aa7-fc28-9d9dc506faa6","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:07.182077Z","iopub.execute_input":"2025-06-02T02:47:07.182406Z","iopub.status.idle":"2025-06-02T02:47:07.561098Z","shell.execute_reply.started":"2025-06-02T02:47:07.182386Z","shell.execute_reply":"2025-06-02T02:47:07.560223Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Data Preparation","metadata":{"id":"VorHfLMORKin"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.transforms import functional as F\nimport torchvision.transforms as T\nfrom PIL import Image\nimport os\nimport xml.etree.ElementTree as ET\nfrom sklearn.model_selection import train_test_split\nimport random\nimport numpy as np","metadata":{"id":"pPEOJkbYR4wm","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:07.564181Z","iopub.execute_input":"2025-06-02T02:47:07.564453Z","iopub.status.idle":"2025-06-02T02:47:08.489914Z","shell.execute_reply.started":"2025-06-02T02:47:07.564433Z","shell.execute_reply":"2025-06-02T02:47:08.488649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set random seed for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)","metadata":{"id":"ExTHzHuZSBLi","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:08.490751Z","iopub.execute_input":"2025-06-02T02:47:08.491263Z","iopub.status.idle":"2025-06-02T02:47:08.503263Z","shell.execute_reply.started":"2025-06-02T02:47:08.491239Z","shell.execute_reply":"2025-06-02T02:47:08.502408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define custom dataset class\nclass SelfDrivingCarsDataset(Dataset):\n    def __init__(self, dataframe, img_dir, transforms=None):\n        self.dataframe = dataframe\n        self.img_dir = img_dir\n        self.transforms = transforms\n        self.image_filenames = self.dataframe['frame'].unique()\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        img_filename = self.image_filenames[idx]\n        img_path = os.path.join(self.img_dir, img_filename)\n        img = Image.open(img_path).convert(\"RGB\")\n\n        # Get annotations for the current image\n        img_annotations = self.dataframe[self.dataframe['frame'] == img_filename]\n\n        boxes = []\n        labels = []\n        for index, row in img_annotations.iterrows():\n            # 'x_min', 'y_min', 'x_max', 'y_max' are the columns for bounding box coordinates\n            # 'class_id' is the column for the class label (must be int and start from 1)\n            x_min, y_min, x_max, y_max = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n            boxes.append([x_min, y_min, x_max, y_max])\n            labels.append(int(row['class_id'])) # Ensure labels are integers\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n        target[\"filename\"] = img_filename # Add the filename to the target\n\n        if self.transforms is not None:\n            for t in self.transforms:\n              if isinstance(t, T.ToTensor):\n                img = t(img)\n\n        return img, target","metadata":{"id":"XMKk2uUtSFW-","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:08.504499Z","iopub.execute_input":"2025-06-02T02:47:08.504963Z","iopub.status.idle":"2025-06-02T02:47:08.528463Z","shell.execute_reply.started":"2025-06-02T02:47:08.504936Z","shell.execute_reply":"2025-06-02T02:47:08.527373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define transformation function (can be expanded for data augmentation)\ndef get_transform(train):\n    transforms = []\n    # Convert PIL Image to PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # Add more transformations for training (e.g., horizontal flip)\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return transforms\n","metadata":{"id":"w4etrk9CSI_z","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:08.529461Z","iopub.execute_input":"2025-06-02T02:47:08.529691Z","iopub.status.idle":"2025-06-02T02:47:08.55107Z","shell.execute_reply.started":"2025-06-02T02:47:08.529674Z","shell.execute_reply":"2025-06-02T02:47:08.550098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Helper function to collate data\ndef collate_fn(batch):\n    return tuple(zip(*batch))","metadata":{"id":"6DersejLSR61","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:08.552044Z","iopub.execute_input":"2025-06-02T02:47:08.552368Z","iopub.status.idle":"2025-06-02T02:47:08.573607Z","shell.execute_reply.started":"2025-06-02T02:47:08.552336Z","shell.execute_reply":"2025-06-02T02:47:08.572682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split data into training and testing sets\ntrain_df, test_df = train_test_split(newData, test_size=0.2, random_state=SEED)\n\n# Declare image directory\nimg_directory = '/kaggle/input/self-driving-cars/images' # Update this path if the image path are in different directory\n\n# Create datasets\ntrain_dataset = SelfDrivingCarsDataset(train_df, img_directory, transforms=get_transform(train=True))\ntest_dataset = SelfDrivingCarsDataset(test_df, img_directory, transforms=get_transform(train=False))","metadata":{"id":"GDWhOgItSVAw","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:08.574528Z","iopub.execute_input":"2025-06-02T02:47:08.574829Z","iopub.status.idle":"2025-06-02T02:47:08.631257Z","shell.execute_reply.started":"2025-06-02T02:47:08.574802Z","shell.execute_reply":"2025-06-02T02:47:08.630349Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Model Setup","metadata":{"id":"535WDtoCWil5"}},{"cell_type":"code","source":"# Create data loaders\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=4, # Adjust batch size as needed\n    shuffle=True,\n    num_workers=2, # Adjust based on your system's capabilities\n    collate_fn=collate_fn\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=1, # Batch size for testing is often 1\n    shuffle=False,\n    num_workers=2,\n    collate_fn=collate_fn\n)\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()","metadata":{"id":"WZypzTJDTDaT","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:08.632326Z","iopub.execute_input":"2025-06-02T02:47:08.632575Z","iopub.status.idle":"2025-06-02T02:47:08.638511Z","shell.execute_reply.started":"2025-06-02T02:47:08.632555Z","shell.execute_reply":"2025-06-02T02:47:08.637649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load a pre-trained Faster R-CNN model\nmodel = fasterrcnn_resnet50_fpn(pretrained=True)\n\n# Replace the classifier with a new one, that has\n# num_classes which is the number of background + classes\nnum_classes = newData['class_id'].nunique() + 1 # Add 1 for background class\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# Move model to the appropriate device\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)","metadata":{"id":"me2rHTy3Tcpj","outputId":"ec485060-4666-4968-8fba-60465fcdbe48","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:08.639535Z","iopub.execute_input":"2025-06-02T02:47:08.639879Z","iopub.status.idle":"2025-06-02T02:47:10.581489Z","shell.execute_reply.started":"2025-06-02T02:47:08.63985Z","shell.execute_reply":"2025-06-02T02:47:10.580596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define optimizer and learning rate scheduler\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n\nprint(f\"Training on {device}\")\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of testing samples: {len(test_dataset)}\")\nprint(f\"Number of classes (including background): {num_classes}\")","metadata":{"id":"Q1nyaUpoRYWX","outputId":"5a8bf487-8592-4fae-8848-6580cca16f97","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:10.582853Z","iopub.execute_input":"2025-06-02T02:47:10.583163Z","iopub.status.idle":"2025-06-02T02:47:10.590912Z","shell.execute_reply.started":"2025-06-02T02:47:10.583136Z","shell.execute_reply":"2025-06-02T02:47:10.59Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Faster R-CNN Training","metadata":{"id":"DwqubeC0XTaG"}},{"cell_type":"code","source":"import gc\n\ndef train_model(model, optimizer, data_loader, device, epoch):\n    model.train()\n    running_loss = 0.0\n\n    # Initialize running totals for individual loss components\n    running_classifier_loss = 0.0\n    running_box_reg_loss = 0.0\n    running_objectness_loss = 0.0\n    running_rpn_box_reg_loss = 0.0\n\n    print(f\"Epoch {epoch+1}\")\n\n    # Wrap your data loader with tqdm for a progress bar\n    from tqdm import tqdm\n    data_loader_tqdm = tqdm(data_loader, desc=f\"Training Epoch {epoch+1}\")\n\n    for i, (images, targets) in enumerate(data_loader_tqdm):\n        images = list(image.to(device) for image in images)\n         \n        targets_on_device = []\n        for t in targets:\n            target_on_device = {}\n            # Iterate through the expected keys that are tensors\n            # These are typically 'boxes', 'labels', 'image_id', 'area', 'iscrowd'\n            for key in ['boxes', 'labels', 'image_id', 'area', 'iscrowd']:\n                 if key in t:\n                     # Ensure it's a tensor and move it\n                     if isinstance(t[key], torch.Tensor):\n                         target_on_device[key] = t[key].to(device)\n                     else:\n                         # This case shouldn't ideally happen for these keys,\n                         # but include a safeguard or a warning if needed.\n                         print(f\"Warning: Target key '{key}' is not a tensor for image.\") # Optional warning\n                         target_on_device[key] = t[key] # Keep on CPU if not a tensor\n                 else:\n                     print(f\"Warning: Target key '{key}' not found in target dictionary.\") # Optional warning\n\n            # Also add the non-tensor data like filename\n            if 'filename' in t:\n                 target_on_device['filename'] = t['filename']\n\n            targets_on_device.append(target_on_device)\n\n        loss_dict = model(images, targets_on_device)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        # Accumulate total loss and individual loss components\n        running_loss += losses.item()\n        running_classifier_loss += loss_dict['loss_classifier'].item()\n        running_box_reg_loss += loss_dict['loss_box_reg'].item()\n        running_objectness_loss += loss_dict['loss_objectness'].item()\n        running_rpn_box_reg_loss += loss_dict['loss_rpn_box_reg'].item()\n\n        # Update the progress bar description with current batch loss\n        data_loader_tqdm.set_postfix({\n            'batch_loss': losses.item(),\n            'avg_epoch_loss': running_loss / (i + 1),\n            'avg_cls_loss': running_classifier_loss / (i + 1),\n            'avg_box_loss': running_box_reg_loss / (i + 1),\n            'avg_obj_loss': running_objectness_loss / (i + 1),\n            'avg_rpn_box_loss': running_rpn_box_reg_loss / (i + 1)\n        })\n\n\n    # Calculate average losses for the epoch\n    epoch_loss = running_loss / len(data_loader)\n    epoch_classifier_loss = running_classifier_loss / len(data_loader)\n    epoch_box_reg_loss = running_box_reg_loss / len(data_loader)\n    epoch_objectness_loss = running_objectness_loss / len(data_loader)\n    epoch_rpn_box_reg_loss = running_rpn_box_reg_loss / len(data_loader)\n\n\n    print(f\"Epoch {epoch+1} finished:\")\n    print(f\"  Average Total Loss: {epoch_loss:.4f}\")\n    print(f\"  Average Classifier Loss: {epoch_classifier_loss:.4f}\")\n    print(f\"  Average Bounding Box Regression Loss: {epoch_box_reg_loss:.4f}\")\n    print(f\"  Average Objectness Loss: {epoch_objectness_loss:.4f}\")\n    print(f\"  Average RPN Bounding Box Regression Loss: {epoch_rpn_box_reg_loss:.4f}\")\n\nprint(f\"Starting training... on {device}\")\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ngc.collect()","metadata":{"id":"lbOvnnGFTlpH","outputId":"f7649aee-2e47-48db-aaa4-61d40cd05f8c","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:10.592087Z","iopub.execute_input":"2025-06-02T02:47:10.59267Z","iopub.status.idle":"2025-06-02T02:47:10.815159Z","shell.execute_reply.started":"2025-06-02T02:47:10.59264Z","shell.execute_reply":"2025-06-02T02:47:10.814297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the number of epochs\nnum_epochs = 5 # You can adjust this, increasing it is recommended\n\nfor epoch in range(num_epochs):\n    train_model(model, optimizer, train_loader, device, epoch)\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    gc.collect()\n    # Update the learning rate\n    lr_scheduler.step()\n\nprint(\"Training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:47:10.816151Z","iopub.execute_input":"2025-06-02T02:47:10.816547Z","iopub.status.idle":"2025-06-02T02:48:15.652712Z","shell.execute_reply.started":"2025-06-02T02:47:10.816519Z","shell.execute_reply":"2025-06-02T02:48:15.650535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Evaluation","metadata":{"id":"dj_RlhZaXboV"}},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nimport gc\nimport json # Or use pandas and save to CSV/Parquet\n\ndef evaluate_and_save_predictions(model, data_loader, device, output_path=\"inference_results.json\"):\n    model.eval()\n    all_results = [] # Accumulate results for saving\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    print(\"Starting inference and saving results...\")\n\n    for i, (images, targets) in enumerate(tqdm(data_loader, desc=\"Performing Inference and Saving\")):\n        images = list(img.to(device) for img in images)\n        # Optional: Move targets to device as well\n        # targets_on_device = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        with torch.no_grad():\n            output = model(images)\n\n        # Process results and prepare for saving (still on CPU after .cpu())\n        for j in range(len(output)):\n            img_filename = targets[j]['filename'] # Get the filename\n            \n            img_predictions = {\n                'image_filename': img_filename, # Use filename as identifier\n                'boxes': output[j]['boxes'].cpu().tolist(), # Convert tensor to list\n                'labels': output[j]['labels'].cpu().tolist(),\n                'scores': output[j]['scores'].cpu().tolist()\n            }\n            all_results.append(img_predictions)\n\n        # Save periodically to avoid accumulating too much in memory\n        if (i + 1) % 100 == 0 or (i + 1) == len(data_loader): # Save every 100 batches or at the end\n            mode = 'w' if i < 100 else 'a' # Write initially, append later\n            with open(output_path, mode) as f:\n                # If appending, need to handle JSON array structure\n                if mode == 'a':\n                     # Read existing, append, and rewrite or use a format suitable for appending\n                     # Simple append might not work for a single JSON array.\n                     # Consider writing each result on a new line and processing line by line later,\n                     # or use a format like JSON Lines (jsonl).\n                     # For simplicity here, let's overwrite for demonstration or use a list of dicts and save once at the end.\n                     # Let's adjust to save the list of dicts at the end, but manage memory by processing one batch at a time.\n                     pass # We will save all_results at the very end.\n\n        # Clear memory after each batch\n        del images, targets, output #, targets_on_device # uncomment if you moved targets to device\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        gc.collect()\n\n    # Save all accumulated results to a single file at the end\n    with open(output_path, 'w') as f:\n        json.dump(all_results, f)\n\n    print(f\"Inference complete. Results saved to {output_path}\")\n    # You would then load this file later to calculate metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:48:15.65363Z","iopub.status.idle":"2025-06-02T02:48:15.654053Z","shell.execute_reply.started":"2025-06-02T02:48:15.653901Z","shell.execute_reply":"2025-06-02T02:48:15.653916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the new evaluation function after the training loop\n# This will perform inference and collect predictions and ground truths\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\ngc.collect()\npredictions, ground_truths = evaluate_and_save_predictions(model, test_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:48:15.65553Z","iopub.status.idle":"2025-06-02T02:48:15.655852Z","shell.execute_reply.started":"2025-06-02T02:48:15.655701Z","shell.execute_reply":"2025-06-02T02:48:15.655715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport pandas as pd\n\n# Load predictions from the saved file\nwith open('/kaggle/input/inference-result/inference_results.json', 'r') as f:\n    all_predictions = json.load(f)\n\n# Load the ground truth data for the test set\nground_truth_df = test_df.copy() # Make a copy to avoid modifying the original","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:59:49.159774Z","iopub.execute_input":"2025-06-02T02:59:49.160116Z","iopub.status.idle":"2025-06-02T02:59:50.271983Z","shell.execute_reply.started":"2025-06-02T02:59:49.16009Z","shell.execute_reply":"2025-06-02T02:59:50.270999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision.ops import box_iou\nimport pandas as pd\nimport numpy as np # Import numpy\n\ndef match_predictions_to_ground_truth(predictions, ground_truth_df, iou_threshold=0.5):\n    matched_results = []\n\n    # Get unique class IDs from the ground truth to ensure all classes are represented\n    # Add a \"Background\" label (often represented by class ID 0 or a special index)\n    # Assuming class IDs are 1-based, let's use 0 for background/no object\n    unique_classes = sorted(ground_truth_df['class_id'].unique().tolist())\n    # class_labels = ['Background'] + [f'Class {c}' for c in unique_classes]\n    # Let's stick to class IDs for the matrix indexing for now\n\n    for prediction in predictions:\n        image_filename = prediction['image_filename']\n        predicted_boxes = torch.tensor(prediction['boxes'], dtype=torch.float32)\n        predicted_labels = torch.tensor(prediction['labels'], dtype=torch.int64)\n        predicted_scores = torch.tensor(prediction['scores'], dtype=torch.float32)\n\n        img_ground_truth = ground_truth_df[ground_truth_df['frame'] == image_filename]\n\n        gt_boxes = torch.tensor(img_ground_truth[['xmin', 'ymin', 'xmax', 'ymax']].values, dtype=torch.float32)\n        gt_labels = torch.tensor(img_ground_truth['class_id'].values, dtype=torch.int64)\n\n        # Track matched ground truth indices for False Negatives\n        matched_gt_indices = -torch.ones(len(gt_boxes), dtype=torch.long)\n\n        if len(predicted_boxes) > 0:\n            if len(gt_boxes) > 0:\n                iou_matrix = box_iou(predicted_boxes, gt_boxes)\n\n                # Sort predictions by confidence score in descending order\n                sorted_scores, sorted_indices = predicted_scores.sort(descending=True)\n\n                for i in sorted_indices:\n                    pred_box = predicted_boxes[i]\n                    pred_label = predicted_labels[i]\n                    pred_score = predicted_scores[i]\n\n                    # Find the best ground truth match for this prediction\n                    best_iou = 0\n                    best_gt_idx = -1\n\n                    for gt_idx in range(len(gt_boxes)):\n                        # Check if the ground truth box is not already matched\n                        if matched_gt_indices[gt_idx] == -1:\n                            current_iou = iou_matrix[i, gt_idx]\n                            if current_iou > best_iou and current_iou >= iou_threshold:\n                                best_iou = current_iou\n                                best_gt_idx = gt_idx\n\n                    if best_gt_idx != -1:\n                        # Matched to a ground truth box. Record both predicted and ground truth labels.\n                        matched_results.append({\n                            'image_filename': image_filename,\n                            'predicted_box': pred_box.tolist(),\n                            'predicted_label': pred_label.item(),\n                            'predicted_score': pred_score.item(),\n                            'actual_label': gt_labels[best_gt_idx].item(), # Actual ground truth label\n                            'is_tp': True,\n                            'is_fn': False,\n                            'is_fp': False,\n                            'matched_gt_box': gt_boxes[best_gt_idx].tolist()\n                        })\n                        matched_gt_indices[best_gt_idx] = i # Mark ground truth as matched\n                    else:\n                        # No match with IoU > threshold or all potential ground truths already matched\n                        # This prediction is a False Positive.\n                        matched_results.append({\n                            'image_filename': image_filename,\n                            'predicted_box': pred_box.tolist(),\n                            'predicted_label': pred_label.item(),\n                            'predicted_score': pred_score.item(),\n                            'actual_label': None, # No corresponding ground truth\n                            'is_tp': False,\n                            'is_fn': False,\n                            'is_fp': True,\n                            'matched_gt_box': None\n                        })\n\n            elif len(gt_boxes) == 0:\n                 # Predictions but no ground truth for this image -> all are False Positives\n                for i in range(len(predicted_boxes)):\n                     matched_results.append({\n                        'image_filename': image_filename,\n                        'predicted_box': predicted_boxes[i].tolist(),\n                        'predicted_label': predicted_labels[i].item(),\n                        'predicted_score': predicted_scores[i].item(),\n                        'actual_label': None,\n                        'is_tp': False,\n                        'is_fn': False,\n                        'is_fp': True,\n                        'matched_gt_box': None\n                     })\n\n        # Identify False Negatives (ground truth boxes not matched by any prediction)\n        for gt_idx in range(len(gt_boxes)):\n            if matched_gt_indices[gt_idx] == -1:\n                 matched_results.append({\n                    'image_filename': image_filename,\n                    'predicted_box': None, # No predicted box for this FN\n                    'predicted_label': None, # No predicted label\n                    'predicted_score': None,\n                    'actual_label': gt_labels[gt_idx].item(), # Actual ground truth label\n                    'is_tp': False,\n                    'is_fn': True,\n                    'is_fp': False,\n                    'matched_gt_box': gt_boxes[gt_idx].tolist()\n                 })\n        # If both predicted_boxes and gt_boxes are empty, nothing to add to results\n\n    return pd.DataFrame(matched_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:59:54.85429Z","iopub.execute_input":"2025-06-02T02:59:54.854601Z","iopub.status.idle":"2025-06-02T02:59:54.870245Z","shell.execute_reply.started":"2025-06-02T02:59:54.854577Z","shell.execute_reply":"2025-06-02T02:59:54.869244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the modified function\nmatched_results_df = match_predictions_to_ground_truth(all_predictions, ground_truth_df, iou_threshold=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T03:00:09.72131Z","iopub.execute_input":"2025-06-02T03:00:09.721811Z","iopub.status.idle":"2025-06-02T03:01:27.117553Z","shell.execute_reply.started":"2025-06-02T03:00:09.721777Z","shell.execute_reply":"2025-06-02T03:01:27.116452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(matched_results_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T03:01:30.265257Z","iopub.execute_input":"2025-06-02T03:01:30.265569Z","iopub.status.idle":"2025-06-02T03:01:30.278089Z","shell.execute_reply.started":"2025-06-02T03:01:30.265545Z","shell.execute_reply":"2025-06-02T03:01:30.277031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef calculate_ap(matched_results_df, class_id):\n    # Filter results for the specific class's predictions and ground truths\n    # We need rows that are TPs or FPs with this predicted_label\n    # AND rows that are FNs with this actual_label\n    class_results = matched_results_df[\n        ((matched_results_df['predicted_label'] == class_id) & (matched_results_df['is_tp'] | matched_results_df['is_fp'])) |\n        ((matched_results_df['actual_label'] == class_id) & matched_results_df['is_fn'])\n    ].copy()\n\n    # We only need the rows that are either TPs or FPs for sorting by score.\n    # FNs don't have a score or predicted box.\n    predictions_for_ap = class_results[class_results['predicted_label'] == class_id].copy()\n\n    if predictions_for_ap.empty and class_results[class_results['is_fn']].empty:\n        return 0.0 # AP is 0 if no predictions or ground truth for this class\n\n    # Sort predictions by confidence score in descending order\n    predictions_for_ap.sort_values(by='predicted_score', ascending=False, inplace=True)\n\n    # Calculate cumulative True Positives (TP) and False Positives (FP) based on the sorted predictions\n    predictions_for_ap['tp_cumulative'] = predictions_for_ap['is_tp'].cumsum()\n    predictions_for_ap['fp_cumulative'] = predictions_for_ap['is_fp'].cumsum()\n\n    # Calculate the total number of ground truth objects for this class.\n    # This is the sum of TPs and FNs for this class in the ENTIRE test set results.\n    total_ground_truth = matched_results_df[matched_results_df['actual_label'] == class_id]['is_fn'].sum() + \\\n                         matched_results_df[matched_results_df['actual_label'] == class_id]['is_tp'].sum()\n\n\n    if total_ground_truth == 0:\n        # If there are no ground truth objects for this class, AP is undefined or 0.0 by convention.\n        # If there were predictions but no ground truth, they are all FPs, leading to 0 precision.\n        # If there were no predictions and no ground truth, AP is 0.\n        return 0.0\n\n    # Calculate precision and recall at each prediction point\n    # Handle division by zero for precision if no predictions yet\n    predictions_for_ap['precision'] = predictions_for_ap['tp_cumulative'] / (predictions_for_ap['tp_cumulative'] + predictions_for_ap['fp_cumulative'])\n    predictions_for_ap['recall'] = predictions_for_ap['tp_cumulative'] / total_ground_truth\n\n    # Calculate AP using the 11-point interpolation method (or area under the curve)\n    # Here we'll use the 11-point method as a common approach\n    recall_levels = np.linspace(0, 1, 11)\n    ap = 0\n    for r_level in recall_levels:\n        # Find the maximum precision for recalls greater than or equal to the current recall_level\n        precisions_at_r_level = predictions_for_ap[predictions_for_ap['recall'] >= r_level]['precision']\n        if not precisions_at_r_level.empty:\n            ap += precisions_at_r_level.max()\n    ap /= 11\n\n    return ap\n\n# Get unique class IDs from the ground truth\n# This remains the same as it correctly identifies the classes present in the ground truth\nunique_classes = sorted(ground_truth_df['class_id'].unique().tolist())\n\n# Calculate AP for each class\nap_per_class = {}\nfor class_id in unique_classes:\n    ap_per_class[class_id] = calculate_ap(matched_results_df, class_id)\n\nprint(\"Average Precision (AP) per class (at IoU=0.5):\")\nfor class_id, ap_score in ap_per_class.items():\n    print(f\"  Class {class_id}: {ap_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T03:01:35.926129Z","iopub.execute_input":"2025-06-02T03:01:35.92724Z","iopub.status.idle":"2025-06-02T03:01:36.461989Z","shell.execute_reply.started":"2025-06-02T03:01:35.927204Z","shell.execute_reply":"2025-06-02T03:01:36.461046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate mAP\nif len(ap_per_class) > 0:\n    mean_ap = sum(ap_per_class.values()) / len(ap_per_class)\n    print(f\"\\nMean Average Precision (mAP at IoU=0.5): {mean_ap:.4f}\")\nelse:\n    print(\"\\nCould not calculate mAP as there are no classes with ground truth data.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T03:01:41.14494Z","iopub.execute_input":"2025-06-02T03:01:41.145285Z","iopub.status.idle":"2025-06-02T03:01:41.151067Z","shell.execute_reply.started":"2025-06-02T03:01:41.145259Z","shell.execute_reply":"2025-06-02T03:01:41.150081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate overall precision and recall\ntotal_tp = matched_results_df['is_tp'].sum()\ntotal_fp = matched_results_df['is_fp'].sum()\ntotal_fn = matched_results_df['is_fn'].sum()\n\noverall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\noverall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\noverall_f1_score = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0.0\n\nprint(\"\\nOverall Metrics (at IoU=0.5):\")\nprint(f\"  Total True Positives: {total_tp:.4f}\")\nprint(f\"  Total False Positives: {total_fp:.4f}\")\nprint(f\"  Total False Negatives: {total_fn:.4f}\")\nprint(f\"  Overall Precision: {overall_precision:.4f}\")\nprint(f\"  Overall Recall: {overall_recall:.4f}\")\nprint(f\"  Overall F1-score: {overall_f1_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T03:01:44.627812Z","iopub.execute_input":"2025-06-02T03:01:44.628122Z","iopub.status.idle":"2025-06-02T03:01:44.638076Z","shell.execute_reply.started":"2025-06-02T03:01:44.6281Z","shell.execute_reply":"2025-06-02T03:01:44.637054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the list of all possible class IDs (including background/no object)\nunique_actual_classes = sorted(ground_truth_df['class_id'].unique().tolist())\n# Determine the range of possible class IDs\n# The maximum class ID will define the size of our matrix.\nmax_class_id = max(unique_actual_classes) if unique_actual_classes else 0\nall_possible_class_ids = list(range(max_class_id + 1)) # Includes 0 for background\n\n# Initialize confusion matrix with zeros\n# Rows: Actual Class (including Background)\n# Columns: Predicted Class (including Background)\nconfusion_matrix_counts = np.zeros((len(all_possible_class_ids), len(all_possible_class_ids)), dtype=int)\n\n# Populate the confusion matrix\nfor index, row in matched_results_df.iterrows():\n    actual_label = row['actual_label']\n    predicted_label = row['predicted_label']\n    is_tp = row['is_tp']\n    is_fp = row['is_fp']\n    is_fn = row['is_fn']\n\n    if is_tp:\n        # True Positive: Actual class was 'actual_label', predicted class was 'predicted_label'\n        # Since it's a TP, actual_label and predicted_label should be the same object class\n        if actual_label is not None and predicted_label is not None:\n             # Map actual_label to its index in all_possible_class_ids\n             actual_idx = all_possible_class_ids.index(actual_label)\n             predicted_idx = all_possible_class_ids.index(predicted_label)\n             confusion_matrix_counts[actual_idx, predicted_idx] += 1\n    elif is_fp:\n        # False Positive: Predicted 'predicted_label', but there was no actual object ('Background')\n        if predicted_label is not None:\n            # Actual class is 'Background' (index 0)\n            actual_idx = 0\n            # Predicted class is the predicted object class\n            predicted_idx = all_possible_class_ids.index(predicted_label)\n            confusion_matrix_counts[actual_idx, predicted_idx] += 1\n    elif is_fn:\n        # False Negative: Actual class was 'actual_label', but nothing was predicted ('Background')\n        if actual_label is not None:\n            # Actual class is the actual object class\n            actual_idx = all_possible_class_ids.index(actual_label)\n            # Predicted class is 'Background' (index 0)\n            predicted_idx = 0\n            confusion_matrix_counts[actual_idx, predicted_idx] += 1\n\n# Note: True Negatives (correctly identifying background as background) are not explicitly\n# counted in this object detection context the same way as in classification.\n# The row corresponding to 'Background' in the confusion matrix (index 0) will primarily\n# show False Positives (predicted object when actual is background).\n\nprint(\"\\nConfusion Matrix Counts:\")\nprint(confusion_matrix_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T03:01:48.011319Z","iopub.execute_input":"2025-06-02T03:01:48.011605Z","iopub.status.idle":"2025-06-02T03:02:00.648226Z","shell.execute_reply.started":"2025-06-02T03:01:48.011586Z","shell.execute_reply":"2025-06-02T03:02:00.647187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create labels for the confusion matrix axes\n# Map class IDs back to meaningful names if you have them\n# For now, let's use \"Background\" and \"Class X\"\nconfusion_matrix_labels = ['Background'] + [f'Class {c}' for c in unique_actual_classes]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T03:02:04.224929Z","iopub.execute_input":"2025-06-02T03:02:04.225279Z","iopub.status.idle":"2025-06-02T03:02:04.230572Z","shell.execute_reply.started":"2025-06-02T03:02:04.225253Z","shell.execute_reply":"2025-06-02T03:02:04.229446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T03:02:07.964867Z","iopub.execute_input":"2025-06-02T03:02:07.965195Z","iopub.status.idle":"2025-06-02T03:02:07.970147Z","shell.execute_reply.started":"2025-06-02T03:02:07.965149Z","shell.execute_reply":"2025-06-02T03:02:07.968825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot the confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_matrix_counts, annot=True, fmt='d', cmap='Blues',\n            xticklabels=confusion_matrix_labels, yticklabels=confusion_matrix_labels)\nplt.xlabel('Predicted Class')\nplt.ylabel('Actual Class')\nplt.title('Object Detection Confusion Matrix (IoU=0.5)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T03:02:10.020262Z","iopub.execute_input":"2025-06-02T03:02:10.02136Z","iopub.status.idle":"2025-06-02T03:02:10.350648Z","shell.execute_reply.started":"2025-06-02T03:02:10.021325Z","shell.execute_reply":"2025-06-02T03:02:10.349609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a path to save the model\nmodel_save_path = 'faster_rcnn_cars.pth'\n\n# Save the model's state dictionary\ntorch.save(model.state_dict(), model_save_path)\n\nprint(f\"Model saved to {model_save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:48:15.675775Z","iopub.status.idle":"2025-06-02T02:48:15.676484Z","shell.execute_reply.started":"2025-06-02T02:48:15.675916Z","shell.execute_reply":"2025-06-02T02:48:15.675928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Load Model (Optional)","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nfile =('/kaggle/input/self-driving-cars/labels_train.csv') #Adjust the file path if you are running this besides on kaggle\nnewData = pd.read_csv(file)\nnewData.head()\n\n# Define a path to load the model\nmodel_save_path = 'faster_rcnn_cars.pth'\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# --- To load the model later for inference ---\n\n# First, instantiate a model with the same architecture as the trained one\n# You need to redefine or import the model definition (fasterrcnn_resnet50_fpn with the correct number of classes)\nloaded_model = fasterrcnn_resnet50_fpn(pretrained=False) # No need for pre-training weights\nnum_classes = newData['class_id'].nunique() + 1 # Ensure num_classes is the same as during training\nin_features = loaded_model.roi_heads.box_predictor.cls_score.in_features\nloaded_model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# Load the saved state dictionary into the model\nloaded_model.load_state_dict(torch.load(model_save_path,map_location=device))\n\n# Move the loaded model to the appropriate device (e.g., CPU or GPU)\nloaded_model.to(device)\n\n# Set the model to evaluation mode\nloaded_model.eval()\n\nmodel = loaded_model\n\nprint(\"Model loaded successfully for inference.\")\n\n# Now 'loaded_model' can be used to make predictions on new images.\n# For inference, you would pass new images through this loaded_model\n# and process the outputs (boxes, labels, scores).","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T02:48:15.677551Z","iopub.status.idle":"2025-06-02T02:48:15.677889Z","shell.execute_reply.started":"2025-06-02T02:48:15.677679Z","shell.execute_reply":"2025-06-02T02:48:15.67769Z"}},"outputs":[],"execution_count":null}]}